# lakehouse-dataops-pos-graduacao

Projeto educacional de Lakehouse na AWS com DataOps e CI/CD

ğŸ¯ Objetivo do Projeto Este projeto tem como objetivo demonstrar, na prÃ¡tica: A construÃ§Ã£o de um Lakehouse na AWS A aplicaÃ§Ã£o da cultura DataOps O uso de CI/CD para pipelines de dados A organizaÃ§Ã£o de dados em camadas (Legado, Raw, Trusted e Curated) A entrega de produtos de dados analÃ­ticos prontos para BI O projeto utiliza como fonte inicial um arquivo Excel local, simulando um cenÃ¡rio comum de dados legados em empresas.

ğŸ—ï¸ Arquitetura Geral do Lakehouse O Lakehouse serÃ¡ implementado no Amazon S3 com a seguinte organizaÃ§Ã£o lÃ³gica: pos-graduacao-lakehouse

â”œâ”€â”€ lakehouse-legado

â”œâ”€â”€ lakehouse-raw

â”œâ”€â”€ lakehouse-trusted

â””â”€â”€ lakehouse-curated

Cada camada possui um papel especÃ­fico dentro do ciclo de vida do dado.

ğŸ“š Camadas do Lakehouse 

ğŸ”¹ lakehouse-legado - Representa a origem dos dados ContÃ©m apenas o arquivo Excel original Upload realizado via pipeline local (VS Code) Nenhuma transformaÃ§Ã£o aplicada

ğŸ”¹ lakehouse-raw - Armazena o dado bruto com histÃ³rico completo

Cada execuÃ§Ã£o gera uma nova versÃ£o do arquivo

Registro de timestamp de ingestÃ£o

Permite auditoria e reprocessamento

ğŸ”¹ lakehouse-trusted

Dados padronizados e confiÃ¡veis

ConversÃ£o para Parquet

Tipos de dados corrigidos

Particionamento por DataEmissao (ano/mÃªs/dia)

Base para consumo analÃ­tico

ğŸ”¹ lakehouse-curated

Camada de consumo analÃ­tico

Implementada via CTAS (CREATE TABLE AS SELECT) no Athena

Dados agregados e otimizados

Entregas da Curated neste projeto:

Faturamento por Ano

Faturamento por Vendedor

Quantidade de Vendas por Vendedor

ğŸ”„ Processo ELT

O projeto segue o modelo ELT (Extract, Load, Transform):

Extract ExtraÃ§Ã£o do arquivo Excel local

Load Carga do dado bruto no S3 (camadas Legado e Raw)

Transform PadronizaÃ§Ã£o, agregaÃ§Ãµes e otimizaÃ§Ãµes nas camadas Trusted e Curated

ğŸ§  Cultura DataOps Aplicada

Desde o inÃ­cio, o projeto adota prÃ¡ticas de DataOps:

Versionamento de cÃ³digo com Git

OrganizaÃ§Ã£o padronizada de pipelines

SeparaÃ§Ã£o clara de responsabilidades

Rastreabilidade de alteraÃ§Ãµes

PreparaÃ§Ã£o para automaÃ§Ã£o com CI/CD

Dados tratados como produtos

ğŸ” CI/CD no Projeto

O repositÃ³rio Git serÃ¡ utilizado como base para automaÃ§Ã£o:

CI (Continuous Integration)

ValidaÃ§Ã£o de cÃ³digo

Testes de qualidade de dados

ValidaÃ§Ã£o de SQL

CD (Continuous Deployment)

Deploy controlado de pipelines

ExecuÃ§Ã£o de CTAS no Athena

Auditoria e logs automÃ¡ticos

lakehouse-dataops-pos-graduacao/

â”œâ”€â”€ etl/

â”‚â”œâ”€â”€ legado_to_raw/

â”‚â”œâ”€â”€ raw_to_trusted/

â”‚â””â”€â”€ trusted_to_curated/

â”œâ”€â”€ sql/

â”‚â””â”€â”€ curated_ctas/

â”œâ”€â”€ tests/

â”œâ”€â”€ docs/

â””â”€â”€ README.md

ğŸ› ï¸ Tecnologias Utilizadas AWS

Amazon S3

AWS Glue

AWS Glue Data Catalog

Amazon Athena

AWS IAM

AWS Lake Formation

Amazon CloudWatch

AWS CloudTrail

DataOps & CI/CD

Git / GitHub

CodePipeline / CodeBuild (ou equivalente)

Pytest

ValidaÃ§Ãµes de schema e dados

ğŸ“Œ PÃºblico-alvo

Estudantes de pÃ³s-graduaÃ§Ã£o

Engenheiros de Dados em formaÃ§Ã£o

Profissionais de BI e Analytics

Times que desejam entender DataOps na prÃ¡tica

ğŸš€ PrÃ³ximas Etapas do Projeto

Fase 4 â€“ SeguranÃ§a, IAM e GovernanÃ§a

Fase 5 â€“ IngestÃ£o do dado Legado

Fase 6 â€“ Camada Trusted

Fase 7 â€“ Camada Curated com CTAS

Fase 8 â€“ CI/CD em aÃ§Ã£o

Fase 9 â€“ Observabilidade e OperaÃ§Ã£o

ğŸ“– ObservaÃ§Ã£o Final Este projeto foi desenhado para ser didÃ¡tico, progressivo e alinhado com prÃ¡ticas reais de mercado, permitindo evoluÃ§Ã£o futura para Machine Learning, streaming e arquiteturas mais avanÃ§adas.
